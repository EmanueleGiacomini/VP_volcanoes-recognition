"""
dloader module
The DataLoader class is used to load in memory, previously stored datasets generated by a DataBuilder object
"""

from libs.improc import augmentImages
import numpy as np
import tensorflow as tf
from os import listdir
from os.path import isfile, join, isdir
from PIL import Image

LABELS_PATH = 'labels.csv'

def normalize_image(image: tf.Tensor) -> tf.Tensor:
    return tf.image.per_image_standardization(image)

DT_SHUFFLE_BUF_SIZE = int(1e4)
NUM_CLASSES = 5


def create_datasets(images_train: [tf.Tensor], labels_train: [tf.Tensor]) -> [tf.data.Dataset]:
    """
    Returns a list of tf.data.Dataset . Each tf.data.Dataset is associated with only one label
    Can use this list to further develop a balanced dataset (oversampling)
    :param images_train:
    :param labels_train:
    :return:
    """
    # Create a raw dictionary on where to store the training elements
    raw_dataset = {}
    for i in range(NUM_CLASSES):
        raw_dataset[int(i)] = list()
    for image, label in zip(images_train, labels_train):
        raw_dataset[label].append(image)

    pass


def datasets_from_directory(path: str, callbacks={}) -> [tf.data.Dataset]:
    def read_folder(folder: str, label: int) -> tf.data.Dataset:
        current_path = path + folder + '/'
        img_path_lst = [f for f in listdir(current_path) if isfile(join(current_path, f))]
        tf_images = []
        for img_path in img_path_lst:
            image = tf.keras.preprocessing.image.load_img(current_path + img_path)
            image = tf.keras.preprocessing.image.img_to_array(image)
            image = tf.convert_to_tensor(image, tf.float64)
            image = tf.expand_dims(image, -1)
            tf_images.append(image)
        tf_labels = tf.keras.utils.to_categorical([label for _ in range(len(tf_images))], NUM_CLASSES)
        return tf.data.Dataset.from_tensor_slices((tf_images, tf_labels))
    dataset_lst = [0 for _ in range(NUM_CLASSES)]
    dir_lst = [d for d in listdir(path) if isdir(join(path, d))]
    labels = ['not_present', 'definitely',
              'probably', 'possibly',
              'only_pit']

    for i, dir in enumerate(dir_lst):
        if 'load' in callbacks:
            callbacks['load'](i, len(dir_lst) - 1, 'Loading data')
        # Magic happens here
        dataset_lst[labels.index(dir)] = read_folder(dir, labels.index(dir)).map(lambda x, y: (normalize_image(x), y))
    return dataset_lst


class DataLoader:
    def __init__(self, path: str, augment=False, callbacks={}):
        train_ds_lst = datasets_from_directory(path + 'training_data/', callbacks=callbacks)
        validation_ds_lst = datasets_from_directory(path + 'validation_data/', callbacks=callbacks)
        """
        # Open metadata (labels) file
        with open(path + LABELS_PATH, 'r') as f:
            data = []
            for line in f.readlines():
                data.append(int(line.strip()))
        labels_tf = tf.convert_to_tensor(np.array(data), tf.int16)
        images_tf_raw = []
        # Read images
        for i in range(labels_tf.shape[0]):
            # generate image path
            impath = path + str(i) + '.jpg'
            image = np.array(Image.open(impath))
            image = tf.convert_to_tensor(image, tf.float64)
            image = tf.expand_dims(image, -1)
            images_tf_raw.append(image)
            if 'load' in callbacks:
                callbacks['load'](i, labels_tf.shape[0]-1, 'Loading images')
        # Normalize images
        images_tf = []
        for i, image in enumerate(images_tf_raw):
            images_tf.append(normalize_image(image))
            if 'load' in callbacks:
                callbacks['load'](i, len(images_tf_raw)-1, 'Normalizing images')

        self.images = images_tf
        # OneHotEncode labels
        self.labels = labels_tf
        #self.labels = tf.keras.utils.to_categorical(labels_tf, NUM_CLASSES)
        # Split train and test lists
        TRAIN_SPLIT = int(len(self.images) * train_split)
        images_train = self.images[:TRAIN_SPLIT]
        labels_train = self.labels[:TRAIN_SPLIT]
        images_test = self.images[TRAIN_SPLIT:]
        labels_test = self.labels[TRAIN_SPLIT:]

        dsets = create_datasets(images_train, labels_train)
        # augment training set
        if augment:
            images_train, labels_train = augmentImages(images_train, labels_train)
        # OneHotEncode labels
        labels_train = tf.keras.utils.to_categorical(labels_train, NUM_CLASSES)
        labels_test = tf.keras.utils.to_categorical(labels_test, NUM_CLASSES)

        train_dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))
        test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))
        self.train, self.test = train_dataset, test_dataset
        """
    def open(self):
        return self.train, self.test

from matplotlib import pyplot as plt
from libs.callbacks import load_cb

if __name__ == '__main__':
    loader = DataLoader('./dataset/', callbacks={'load': load_cb})
#    train_set, test_set = loader.open()

#    for x, y in train_set.take(1):
#        print(x)
#        print(f'y : {y}')